Project Title: CodeGuard â€“ AI-Powered Code Security Auditor..

ğŸš€ Project Overview CodeGuard is an AI assistant that reviews source code to detect security vulnerabilities, insecure coding practices, and potential exploits. Using Large Language Models (LLMs) with prompt engineering, it analyzes code snippets and returns structured feedback highlighting risks and suggesting secure fixes.

The project leverages zero-shot, one-shot, multi-shot, dynamic, and chain-of-thought prompting to improve the precision and coverage of security analysis. Additionally, embeddings and vector databases enable context-aware vulnerability detection by matching similar known exploits.

ğŸ”§ Features

Vulnerability Detection â€“ Identifies common security flaws (e.g., SQL Injection, XSS, buffer overflow).

Secure Coding Suggestions â€“ Recommends best practices and fixes for vulnerabilities.

Structured Output â€“ Returns results in JSON with fields: issues, suggestions, overall_security_rating.

Prompt Engineering â€“ Applies zero-shot, one-shot, multi-shot, dynamic, and chain-of-thought prompting.

Similarity Search â€“ Embeddings + vector DB to find related vulnerabilities and past patches.

Function Calling â€“ analyzeSecurity(language, snippet) to automate code security auditing.

ğŸ¯ Tech Stack

Backend: Node.js / Python

LLM: OpenAI / Hugging Face API

Database: Vector DB (Pinecone / FAISS)

Evaluation: Custom testing framework using security metrics and vulnerability benchmarks

ğŸ“ˆ Future Scope

Multi-language support for popular programming languages (Python, JavaScript, Java, C++, etc.)

Integration with CI/CD pipelines (GitHub/GitLab) for automated security reviews on pull requests

Continuous learning from new vulnerability databases and advisories

Thanks for the detailed project outline. You're building **CodeGuard**, an AI-powered code security auditor that applies advanced prompting strategies (Zero/One/Multi-shot, Dynamic, Chain-of-Thought), LLMs, and vector search for vulnerability detection.

To help you **mirror the structure** you've described for **AlgoSage** but tailored for **CodeGuard**, hereâ€™s a refined and **RTFC-aligned prompt schema** for your system:

---

## ğŸ›¡ï¸ CodeGuard â€“ AI Code Security Auditor Prompt Framework (RTFC-Aligned)

---

### ğŸ”¹ System Prompt (Role)

```text
You are an AI Code Security Auditor.

Your role is to analyze the given code snippet for security vulnerabilities. You must identify insecure coding practices, potential exploits, and recommend secure alternatives. Return structured output in **valid JSON** format with the following fields:

- issues: List of detected security flaws.
- suggestions: Recommended secure coding practices or fixes.
- overall_security_rating: One of ["High Risk", "Medium Risk", "Low Risk"].
```

---

### ğŸ”¹ User Prompt (Task + Context)

```text
Review the following {{language}} code for security vulnerabilities and provide feedback as per the defined JSON schema:

{{code_snippet}}
```

---

### ğŸ“Œ RTFC Framework Usage

* **R (Role):** Clearly defined as an **AI Code Security Auditor**.
* **T (Task):** Analyze code to detect security flaws and recommend secure practices.
* **F (Format):** Must return a valid **JSON object** with fields: `issues`, `suggestions`, `overall_security_rating`.
* **C (Context):** Includes code snippet and programming language information (e.g., Python, JavaScript, C++, etc.).

---

### âœ… Example Prompt and Output (RTFC Compliant)

**ğŸ§¾ User Input:**

```text
Review the following Python code for security vulnerabilities:

def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)
```

**ğŸ¤– Expected Output (JSON):**

```json
{
  "issues": [
    "SQL Injection vulnerability due to unsafe string concatenation."
  ],
  "suggestions": [
    "Use parameterized queries or prepared statements to safely handle user input."
  ],
  "overall_security_rating": "High Risk"
}
ğŸ” Zero-Shot Prompting in CodeGuard (Python Only)

In CodeGuard, we use Zero-Shot Prompting to detect security vulnerabilities in Python code without providing any prior examples.
The AI understands the task from the prompt instructions alone â€” no demonstrations needed.
This enables scalable, adaptive, and language-focused auditing across a wide range of Python codebases.

ğŸ”¹ Zero-Shot Prompt Template (Python)

System Prompt:
You are an AI Code Security Auditor. Analyze the given Python code snippet, identify security vulnerabilities, recommend secure coding fixes, and provide an overall security rating.
Always return results in JSON format with the following fields:

issues: List of detected security flaws

suggestions: Recommended security improvements

overall_security_rating: A summary rating (e.g., "High Risk", "Medium Risk", "Low Risk")

User Prompt:
Review the following Python code and provide a structured security audit:

{{code_snippet}}

âœ… Zero-Shot Prompting Examples (Python)
ğŸ” Example 1
def process(data):
    exec(data)


AI Output:

{
  "issues": ["Use of 'exec' with untrusted input can lead to code execution vulnerabilities."],
  "suggestions": ["Avoid using 'exec' on user input. Validate and sanitize data, or refactor logic to avoid dynamic execution."],
  "overall_security_rating": "High Risk"
}
---

ğŸ” One-Shot Prompting in CodeGuard (Python Only)

In CodeGuard, we apply One-Shot Prompting, where the AI is provided with a single example to demonstrate the expected input-output behavior.
This helps guide the model with a concrete example while keeping the prompt concise and flexible.
It enhances the model's performance in detecting Python-specific security vulnerabilities.

ğŸ”¹ One-Shot Prompt Template (Python)

System Prompt:
You are an AI Code Security Auditor. Analyze Python code snippets, detect security vulnerabilities, and suggest secure coding practices.
Return results in JSON format with the following fields:

issues: List of detected security flaws

suggestions: Recommended security improvements

overall_security_rating: A summary rating (e.g., "High Risk", "Medium Risk", "Low Risk")

User Prompt (with one example):

ğŸ§ª Example Input (Python):

password = input("Enter password: ")
if password == "1234":
    print("Access granted")


Example Expected Output:

{
  "issues": ["Hardcoded password detected, which is insecure."],
  "suggestions": ["Use secure password storage and verification methods like hashing with bcrypt."],
  "overall_security_rating": "High Risk"
}

ğŸ” Now analyze this Python code:
def process(data):
    exec(data)


Expected Output (AI Response):

{
  "issues": ["Use of 'exec' with untrusted input can lead to arbitrary code execution."],
  "suggestions": ["Avoid using 'exec' on untrusted input. Refactor the code to use safer alternatives."],
  "overall_security_rating": "High Risk"
}


ğŸ“Œ Why One-Shot Prompting (Python)?

ğŸ“˜ Guidance with flexibility: One clear example helps the AI align with expected output format and tone

ğŸ” Security-focused learning: The example demonstrates a common vulnerability, encouraging consistent detection

ğŸ Python-specific accuracy: Helps the model handle language nuances like exec, eval, and string injection

âš¡ Low overhead: More effective than zero-shot for nuanced issues, yet still lightweight compared to multi-shot prompts


ğŸ”¹ Multi-Shot Prompt

System Prompt:
You are an AI code reviewer. Analyze the given code, detect security vulnerabilities, and suggest fixes.
Always return results in JSON format with three fields: issues, suggestions, and overall_feedback..

User Prompt (with multiple examples):

Example 1 (Python):

def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)


Expected Output:

{
  "issues": ["SQL Injection vulnerability due to unsafe string concatenation."],
  "suggestions": ["Use parameterized queries or prepared statements to prevent SQL injection."],
  "overall_feedback": "Critical security flaw detected due to lack of input sanitization."
}


Example 2 (JavaScript):

eval(userInput);


Expected Output:

{
  "issues": ["Use of eval on user input leads to remote code execution risk."],
  "suggestions": ["Avoid using eval. Use safer alternatives like JSON.parse or direct function calls when possible."],
  "overall_feedback": "High-risk pattern found; using eval on untrusted input is dangerous."
}
------------------------------------------------------------------------------
ğŸ”¹ Dynamic Prompting

ğŸ§  What is Dynamic Prompting?
Dynamic prompting adjusts the prompt in real time using placeholders that are filled with user-specific or runtime dataâ€”like the programming language and code snippet. This allows the LLM to better understand the context and produce more relevant, high-quality results.

âœ… Dynamic Prompt Template

ğŸ”¹ System Prompt Template:

You are a code security auditor. Analyze the following {{language}} code snippet, detect security vulnerabilities, and suggest fixes. 
Return the output as a JSON object with the following structure:

{
  "issues": [List of security issues found],
  "suggestions": [Recommended secure coding practices or fixes],
  "overall_security_rating": "High Risk | Medium Risk | Low Risk"
}

ğŸ”¸ Example Invocation with Filled Template

User Prompt:

Review this JavaScript code:

document.write(userInput);


Final Prompt Sent to Model:

You are a code security auditor. Analyze the following JavaScript code snippet, detect security vulnerabilities, and suggest fixes. 
Return the output as a JSON object with the following structure:

{
  "issues": [List of security issues found],
  "suggestions": [Recommended secure coding practices or fixes],
  "overall_security_rating": "High Risk | Medium Risk | Low Risk"
}

Review this JavaScript code:

document.write(userInput);


Expected Output:

{
  "issues": ["Potential Cross-Site Scripting (XSS) vulnerability."],
  "suggestions": ["Use proper input sanitization or DOMPurify to sanitize input before inserting into the DOM."],
  "overall_security_rating": "Medium Risk"
}

.............
### ğŸ§  Chain-of-Thought Prompting Version (Internal Thinking)

**System Prompt:**

```text
You are a code security auditor. Think step-by-step internally about possible security issues in the code, but output **only** the final result in valid JSON. Do not show your reasoning.
```

**User Prompt:**

```text
Review the following Python code:

def authenticate(user_input):
    if user_input == "admin":
        grant_access()
```

**Expected Output:**

```json
{
  "issues": [
    "Hardcoded admin username check without proper authentication mechanism."
  ],
  "suggestions": [
    "Use a secure authentication system with hashed credentials and role-based access control."
  ],
  "overall_security_rating": "High Risk"
}
```
Evaluation Dataset & Testing Framework (Python Only)
ğŸ“‚ 1. Evaluation Dataset

This dataset includes 2 Python code snippets with known security flaws and expected outputs (summarized). These will be used to test the LLM's ability to reason and respond correctly via CoT prompting.

evaluation_dataset = [
    {
        "id": 1,
        "language": "Python",
        "code": """def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)""",
        "expected_issue": "SQL Injection vulnerability due to unsafe string concatenation."
    },
    {
        "id": 2,
        "language": "Python",
        "code": """def process(data):
    exec(data)""",
        "expected_issue": "Use of exec() on user input allows arbitrary code execution."
    },
    
]

âš–ï¸ 2. Judge Function

This function compares the AI's output with expected results based on:

Correctness: Did it catch the core issue?

Completeness: Did it suggest a meaningful fix?

Format: Is the output valid JSON with all required fields?

Clarity: Is the explanation clear?

import json

def judge(ai_output_str, expected_issue):
    try:
        output = json.loads(ai_output_str)
        if not all(k in output for k in ["issues", "suggestions", "overall_feedback"]):
            return "âŒ Fail â€“ Missing required fields"

        issues = output["issues"]
        suggestions = output["suggestions"]
        feedback = output["overall_feedback"]

        if not isinstance(issues, list) or not isinstance(suggestions, list):
            return "âŒ Fail â€“ Issues and suggestions should be lists"

        if any(expected_issue.lower() in issue.lower() for issue in issues):
            return "âœ… Pass â€“ Correct issue identified"
        elif expected_issue.lower() in feedback.lower():
            return "âœ… Pass â€“ Issue mentioned in feedback"
        else:
            return "âŒ Fail â€“ Core issue not identified"

    except json.JSONDecodeError:
        return "âŒ Fail â€“ Output not valid JSON"

ğŸ§ª 3. Testing Framework

Assumes run_model(prompt) is your function to get AI response (mocked here).

def run_model(prompt):
    """
    Placeholder for model interaction.
    Replace with actual OpenAI/HuggingFace API call or local inference.
    """
    # For testing, return dummy JSON output to trigger judge()
    return json.dumps({
        "issues": ["Dummy issue placeholder"],
        "suggestions": ["Dummy suggestion"],
        "overall_feedback": "Dummy feedback"
    })

def run_tests():
    for test in evaluation_dataset:
        prompt = f"Review the following {test['language']} code:\n\n{test['code']}"
        ai_output = run_model(prompt)
        verdict = judge(ai_output, test["expected_issue"])
        print(f"Test {test['id']} â€” {verdict}")

ğŸ§° Output Example (Real Use)

If run_model() correctly returns this for test 1:

{
  "issues": ["User input is directly concatenated into SQL, leading to injection vulnerability."],
  "suggestions": ["Use parameterized queries or ORM to avoid SQL injection."],
  "overall_feedback": "The code is highly vulnerable to SQL injection attacks."
}


Then judge() will return:

âœ… Pass â€“ Correct issue identified

---

### ğŸ”„ Dynamic Prompting Template

You can dynamically insert language and code to scale multi-language auditing:

```text
System Prompt:
You are a code security auditor. Analyze the following {{language}} code, detect vulnerabilities, and suggest secure fixes. Return structured JSON with `issues`, `suggestions`, and `overall_security_rating`.

User Prompt:
Review this {{language}} code:

{{code_snippet}}
```

---

Evaluation Dataset & Testing Framework (Python Only)
ğŸ“‚ 1. Evaluation Dataset

This dataset includes 5 Python code snippets with known security flaws and expected outputs (summarized). These will be used to test the LLM's ability to reason and respond correctly via CoT prompting.

evaluation_dataset = [
    {
        "id": 1,
        "language": "Python",
        "code": """def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)""",
        "expected_issue": "SQL Injection vulnerability due to unsafe string concatenation."
    },
    {
        "id": 2,
        "language": "Python",
        "code": """def process(data):
    exec(data)""",
        "expected_issue": "Use of exec() on user input allows arbitrary code execution."
    },
    {
        "id": 3,
        "language": "Python",
        "code": """import os
os.system("rm -rf " + user_input)""",
        "expected_issue": "Command injection via unsanitized user input in os.system."
    },
    {
        "id": 4,
        "language": "Python",
        "code": """def is_even(n):
    if n % 2 == 1:
        return True
    return False""",
        "expected_issue": "Logic flaw: checks for odd numbers instead of even."
    },
    {
        "id": 5,
        "language": "Python",
        "code": """password = input("Enter password: ")
if password == "admin123":
    print("Access granted")""",
        "expected_issue": "Hardcoded password comparison is insecure."
    }
]

âš–ï¸ 2. Judge Function

This function compares the AI's output with expected results based on:

Correctness: Did it catch the core issue?

Completeness: Did it suggest a meaningful fix?

Format: Is the output valid JSON with all required fields?

Clarity: Is the explanation clear?

import json

def judge(ai_output_str, expected_issue):
    try:
        output = json.loads(ai_output_str)
        if not all(k in output for k in ["issues", "suggestions", "overall_feedback"]):
            return "âŒ Fail â€“ Missing required fields"

        issues = output["issues"]
        suggestions = output["suggestions"]
        feedback = output["overall_feedback"]

        if not isinstance(issues, list) or not isinstance(suggestions, list):
            return "âŒ Fail â€“ Issues and suggestions should be lists"

        if any(expected_issue.lower() in issue.lower() for issue in issues):
            return "âœ… Pass â€“ Correct issue identified"
        elif expected_issue.lower() in feedback.lower():
            return "âœ… Pass â€“ Issue mentioned in feedback"
        else:
            return "âŒ Fail â€“ Core issue not identified"

    except json.JSONDecodeError:
        return "âŒ Fail â€“ Output not valid JSON"

ğŸ§ª 3. Testing Framework

Assumes run_model(prompt) is your function to get AI response (mocked here).

def run_model(prompt):
    """
    Placeholder for model interaction.
    Replace with actual OpenAI/HuggingFace API call or local inference.
    """
    # For testing, return dummy JSON output to trigger judge()
    return json.dumps({
        "issues": ["Dummy issue placeholder"],
        "suggestions": ["Dummy suggestion"],
        "overall_feedback": "Dummy feedback"
    })

def run_tests():
    for test in evaluation_dataset:
        prompt = f"Review the following {test['language']} code:\n\n{test['code']}"
        ai_output = run_model(prompt)
        verdict = judge(ai_output, test["expected_issue"])
        print(f"Test {test['id']} â€” {verdict}")

ğŸ§° Output Example (Real Use)

If run_model() correctly returns this for test 1:

{
  "issues": ["User input is directly concatenated into SQL, leading to injection vulnerability."],
  "suggestions": ["Use parameterized queries or ORM to avoid SQL injection."],
  "overall_feedback": "The code is highly vulnerable to SQL injection attacks."
}


Then judge() will return:

âœ… Pass â€“ Correct issue identified
x


ğŸ¯ One-Shot Prompting System Prompt: You are a code security auditor. Analyze code snippets, detect security issues, and recommend fixes. Return JSON-formatted responses.

User Prompt (with one example):

Example Input (Python):

password = input("Enter password: ") if password == "1234": print("Access granted")

Example Expected Output:

{ "issues": ["Hardcoded password detected, which is insecure."], "suggestions": ["Use secure password storage and verification methods."], "overall_security_rating": "High Risk" }

Now analyze this Java code:

String query = "SELECT * FROM users WHERE id = " + userId; executeQuery(query);

ğŸ¯ Multi-Shot Prompting System Prompt: You are a code security auditor. Analyze code snippets for vulnerabilities and suggest fixes. Return responses as JSON.

User Prompt (with multiple examples):

Example 1 (JavaScript):

document.write(userInput);

Expected Output:

{ "issues": ["Potential Cross-Site Scripting (XSS) vulnerability."], "suggestions": ["Use proper input sanitization before rendering user content."], "overall_security_rating": "Medium Risk" }

Example 2 (C++):

char buffer[10]; strcpy(buffer, input);

Expected Output:

{ "issues": ["Buffer overflow risk due to unsafe strcpy usage."], "suggestions": ["Use safer functions like strncpy or bounds checking."], "overall_security_rating": "High Risk" }

Now review this Python snippet:

import os os.system("rm -rf " + user_input)

ğŸ”¹ Dynamic Prompting System Prompt Template: You are a code security auditor. Analyze the following {{language}} code snippet, detect vulnerabilities, and suggest fixes. Return JSON with issues, suggestions, and overall_security_rating.

User Prompt (dynamic):

Review this {{language}} code:

{{code_snippet}}

Example with Python snippet:

Review this Python code:

def process(data): exec(data)

ğŸ¯ Chain-of-Thought Prompting System Prompt: You are a code security auditor. Think step-by-step internally about possible security issues in the code, then output only the JSON result with issues, suggestions, and overall_security_rating. Do not show your reasoning.

User Prompt: Review the following Python code:

def authenticate(user_input): if user_input == "admin": grant_access()

Expected output:

{ "issues": ["Hardcoded admin username check without proper authentication."], "suggestions": ["Implement secure authentication mechanisms with password hashing."], "overall_security_rating": "High Risk" }

ğŸ§ª Evaluation Dataset & Testing Framework

We created a dataset of 5+ code snippets with known security vulnerabilities and expected outputs to benchmark CodeGuard.

ğŸ“‚ Evaluation Dataset (5 Samples)

[ { "id": 1, "language": "Python", "code": "def login(user_input): query = "SELECT * FROM users WHERE username = '" + user_input + "'"; execute(query)", "expected": "SQL Injection vulnerability due to unsafe string concatenation." }, { "id": 2, "language": "JavaScript", "code": "eval(userInput);", "expected": "Use of eval on user input leads to remote code execution risk." }, { "id": 3, "language": "Java", "code": "String password = "12345";", "expected": "Hardcoded password detected." }, { "id": 4, "language": "C++", "code": "char buf[8]; strcpy(buf, input);", "expected": "Buffer overflow risk with unsafe strcpy." }, { "id": 5, "language": "Python", "code": "os.system('rm -rf ' + user_input)", "expected": "Command injection vulnerability due to unsafe shell command." } ]

ğŸ§‘â€âš–ï¸ Judge Prompt You are a judge. Compare AI output to expected results on:

Correctness â€“ Did the model detect the main security flaw?

Completeness â€“ Did it suggest meaningful fixes?

Format â€“ Is the output valid JSON with all fields?

Clarity â€“ Is the feedback clear and actionable?

Return: Pass / Fail with justification.

âš™ï¸ Testing Framework Setup

for test in dataset: ai_output = run_model(test["code"]) verdict = judge(ai_output, test["expected"]) print(f"Test {test['id']}: {verdict}")

âœ… Why This Setup?

Objective and automated evaluation of security review accuracy.

Clear metrics on detection and suggestion quality.

Easily extendable with new vulnerability cases.


âœ… How to Log Token Usage in CodeGuard
ğŸ“Œ What is a Token?

A token is a basic unit of text that an AI model reads and writes. Tokens can be:

Words (e.g., "hello")

Word parts (e.g., "security" â†’ "secur", "ity")

Punctuation (e.g., "(", ")", ":")

Quick rule of thumb:

1 token â‰ˆ 4 characters in English

100 tokens â‰ˆ 75 words

LLM APIs (like OpenAI) charge per token and have token limits

ğŸ› ï¸ Add Token Logging to CodeGuardâ€™s Backend

If you're using Python for CodeGuard (as listed in your tech stack), here's how to implement token tracking and logging using the openai and tiktoken libraries.

ğŸ§± 1. Token Counting Utility
import openai
import tiktoken  # Install with `pip install tiktoken`

def count_tokens(model, prompt, completion):
    """Logs the number of tokens used in the prompt and completion."""
    encoding = tiktoken.encoding_for_model(model)

    input_tokens = len(encoding.encode(prompt))
    output_tokens = len(encoding.encode(completion))
    total = input_tokens + output_tokens

    print("\nğŸ“¦ Token Usage Summary:")
    print(f"ğŸ“ Input Tokens: {input_tokens}")
    print(f"ğŸ“¤ Output Tokens: {output_tokens}")
    print(f"ğŸ”¢ Total Tokens Used: {total}")
    print("â„¹ï¸ Note: 1 token â‰ˆ 0.75 words. Used for billing and model limits.\n")

    return total

âš™ï¸ 2. Integrate into CodeGuard LLM Call

Assuming you're calling OpenAI like this:

def analyze_security_code(language, code_snippet, model="gpt-4"):
    system_prompt = (
        "You are an AI Code Security Auditor. Analyze the given code snippet, "
        "identify security vulnerabilities, recommend fixes, and provide an overall "
        "security rating. Return results in JSON format containing:\n\n"
        "issues: List of detected security flaws\n"
        "suggestions: Recommended security improvements\n"
        "overall_security_rating: A summary rating (e.g., 'High Risk', 'Medium Risk', 'Low Risk')"
    )
    
    user_prompt = f"Review the following {language} code for security issues:\n\n{code_snippet}"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0.1,
    )

    output = response['choices'][0]['message']['content']
    
    # ğŸ” Log token usage
    count_tokens(model, user_prompt, output)
    
    return output

ğŸ§ª Example Console Output After Each Call
ğŸ“¦ Token Usage Summary:
ğŸ“ Input Tokens: 83
ğŸ“¤ Output Tokens: 127
ğŸ”¢ Total Tokens Used: 210
â„¹ï¸ Note: 1 token â‰ˆ 0.75 words. Used for billing and model limits.

ğŸ“¦ Benefits for CodeGuard

Cost Awareness: Tracks per-audit token cost for billing

Prompt Optimization: Adjust dynamic/multi-shot prompts if they consume too many tokens

Debugging: Useful for tracing why a prompt might hit max token limits

ğŸ” Optional: Log to File Instead of Console
def log_tokens_to_file(file_path, model, prompt, completion):
    total = count_tokens(model, prompt, completion)
    with open(file_path, 'a') as f:
        f.write(f"Model: {model}\n")
        f.write(f"Input Tokens: {len(prompt)}\n")
        f.write(f"Output Tokens: {len(completion)}\n")
        f.write(f"Total Tokens Used: {total}\n\n")
