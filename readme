Project Title: CodeGuard ‚Äì AI-Powered Code Security Auditor.

üöÄ Project Overview CodeGuard is an AI assistant that reviews source code to detect security vulnerabilities, insecure coding practices, and potential exploits. Using Large Language Models (LLMs) with prompt engineering, it analyzes code snippets and returns structured feedback highlighting risks and suggesting secure fixes.

The project leverages zero-shot, one-shot, multi-shot, dynamic, and chain-of-thought prompting to improve the precision and coverage of security analysis. Additionally, embeddings and vector databases enable context-aware vulnerability detection by matching similar known exploits.

üîß Features

Vulnerability Detection ‚Äì Identifies common security flaws (e.g., SQL Injection, XSS, buffer overflow).

Secure Coding Suggestions ‚Äì Recommends best practices and fixes for vulnerabilities.

Structured Output ‚Äì Returns results in JSON with fields: issues, suggestions, overall_security_rating.

Prompt Engineering ‚Äì Applies zero-shot, one-shot, multi-shot, dynamic, and chain-of-thought prompting.

Similarity Search ‚Äì Embeddings + vector DB to find related vulnerabilities and past patches.

Function Calling ‚Äì analyzeSecurity(language, snippet) to automate code security auditing.

üéØ Tech Stack

Backend: Node.js / Python

LLM: OpenAI / Hugging Face API

Database: Vector DB (Pinecone / FAISS)

Evaluation: Custom testing framework using security metrics and vulnerability benchmarks

üìà Future Scope

Multi-language support for popular programming languages (Python, JavaScript, Java, C++, etc.)

Integration with CI/CD pipelines (GitHub/GitLab) for automated security reviews on pull requests

Continuous learning from new vulnerability databases and advisories

Thanks for the detailed project outline. You're building **CodeGuard**, an AI-powered code security auditor that applies advanced prompting strategies (Zero/One/Multi-shot, Dynamic, Chain-of-Thought), LLMs, and vector search for vulnerability detection.

To help you **mirror the structure** you've described for **AlgoSage** but tailored for **CodeGuard**, here‚Äôs a refined and **RTFC-aligned prompt schema** for your system:

---

## üõ°Ô∏è CodeGuard ‚Äì AI Code Security Auditor Prompt Framework (RTFC-Aligned)

---

### üîπ System Prompt (Role)

```text
You are an AI Code Security Auditor.

Your role is to analyze the given code snippet for security vulnerabilities. You must identify insecure coding practices, potential exploits, and recommend secure alternatives. Return structured output in **valid JSON** format with the following fields:

- issues: List of detected security flaws.
- suggestions: Recommended secure coding practices or fixes.
- overall_security_rating: One of ["High Risk", "Medium Risk", "Low Risk"].
```

---

### üîπ User Prompt (Task + Context)

```text
Review the following {{language}} code for security vulnerabilities and provide feedback as per the defined JSON schema:

{{code_snippet}}
```

---

### üìå RTFC Framework Usage

* **R (Role):** Clearly defined as an **AI Code Security Auditor**.
* **T (Task):** Analyze code to detect security flaws and recommend secure practices.
* **F (Format):** Must return a valid **JSON object** with fields: `issues`, `suggestions`, `overall_security_rating`.
* **C (Context):** Includes code snippet and programming language information (e.g., Python, JavaScript, C++, etc.).

---

### ‚úÖ Example Prompt and Output (RTFC Compliant)

**üßæ User Input:**

```text
Review the following Python code for security vulnerabilities:

def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)
```

**ü§ñ Expected Output (JSON):**

```json
{
  "issues": [
    "SQL Injection vulnerability due to unsafe string concatenation."
  ],
  "suggestions": [
    "Use parameterized queries or prepared statements to safely handle user input."
  ],
  "overall_security_rating": "High Risk"
}

üîê Zero-Shot Prompting in CodeGuard (Python Only)

In CodeGuard, we use Zero-Shot Prompting to detect security vulnerabilities in Python code without providing any prior examples.
The AI understands the task from the prompt instructions alone ‚Äî no demonstrations needed.
This enables scalable, adaptive, and language-focused auditing across a wide range of Python codebases.

üîπ Zero-Shot Prompt Template (Python)

System Prompt:
You are an AI Code Security Auditor. Analyze the given Python code snippet, identify security vulnerabilities, recommend secure coding fixes, and provide an overall security rating.
Always return results in JSON format with the following fields:

issues: List of detected security flaws

suggestions: Recommended security improvements

overall_security_rating: A summary rating (e.g., "High Risk", "Medium Risk", "Low Risk")

User Prompt:
Review the following Python code and provide a structured security audit:

{{code_snippet}}

‚úÖ Zero-Shot Prompting Examples (Python)
üîé Example 1
def login(user_input):
    query = "SELECT * FROM users WHERE username = '" + user_input + "'"
    execute(query)


AI Output:

{
  "issues": ["SQL Injection vulnerability due to unsafe string concatenation."],
  "suggestions": ["Use parameterized queries or ORM methods to avoid SQL injection."],
  "overall_security_rating": "High Risk"
}

üîé Example 2
import os
os.system("rm -rf " + user_input)


AI Output:

{
  "issues": ["Command injection vulnerability due to unvalidated shell input."],
  "suggestions": ["Use the 'subprocess' module with argument lists instead of shell=True. Always sanitize input."],
  "overall_security_rating": "High Risk"
}

üîé Example 3
def process(data):
    exec(data)


AI Output:

{
  "issues": ["Use of 'exec' with untrusted input can lead to code execution vulnerabilities."],
  "suggestions": ["Avoid using 'exec' on user input. Validate and sanitize data, or refactor logic to avoid dynamic execution."],
  "overall_security_rating": "High Risk"
}
---

### üß† Chain-of-Thought Prompting Version (Internal Thinking)

**System Prompt:**

```text
You are a code security auditor. Think step-by-step internally about possible security issues in the code, but output **only** the final result in valid JSON. Do not show your reasoning.
```

**User Prompt:**

```text
Review the following Python code:

def authenticate(user_input):
    if user_input == "admin":
        grant_access()
```

**Expected Output:**

```json
{
  "issues": [
    "Hardcoded admin username check without proper authentication mechanism."
  ],
  "suggestions": [
    "Use a secure authentication system with hashed credentials and role-based access control."
  ],
  "overall_security_rating": "High Risk"
}
```

---

### üîÑ Dynamic Prompting Template

You can dynamically insert language and code to scale multi-language auditing:

```text
System Prompt:
You are a code security auditor. Analyze the following {{language}} code, detect vulnerabilities, and suggest secure fixes. Return structured JSON with `issues`, `suggestions`, and `overall_security_rating`.

User Prompt:
Review this {{language}} code:

{{code_snippet}}
```

---

### üìä Testing Framework (Python Pseudo-code)

```python
for test in dataset:
    ai_output = run_model(test["language"], test["code"])
    verdict = judge(ai_output, test["expected"])
    print(f"Test {test['id']}: {verdict}")
```

**Judge Criteria:**

* ‚úÖ **Correctness**: Was the main vulnerability correctly identified?
* ‚úÖ **Completeness**: Were actionable and comprehensive suggestions provided?
* ‚úÖ **Format**: Was the response in valid JSON?
* ‚úÖ **Clarity**: Was the output understandable and useful?

---

### üß† Sample Evaluation Dataset (for Benchmarking)

```json
[
  {
    "id": 1,
    "language": "Python",
    "code": "def login(user_input): query = \"SELECT * FROM users WHERE username = '\" + user_input + \"'\"; execute(query)",
    "expected": "SQL Injection vulnerability due to unsafe string concatenation."
  },
  {
    "id": 2,
    "language": "JavaScript",
    "code": "eval(userInput);",
    "expected": "Use of eval on user input leads to remote code execution risk."
  },
  {
    "id": 3,
    "language": "Java",
    "code": "String password = \"12345\";",
    "expected": "Hardcoded password detected."
  },
  {
    "id": 4,
    "language": "C++",
    "code": "char buf[8]; strcpy(buf, input);",
    "expected": "Buffer overflow risk with unsafe strcpy."
  },
  {
    "id": 5,
    "language": "Python",
    "code": "os.system('rm -rf ' + user_input)",
    "expected": "Command injection vulnerability due to unsafe shell command."
  }
]
```

---



üéØ One-Shot Prompting System Prompt: You are a code security auditor. Analyze code snippets, detect security issues, and recommend fixes. Return JSON-formatted responses.

User Prompt (with one example):

Example Input (Python):

password = input("Enter password: ") if password == "1234": print("Access granted")

Example Expected Output:

{ "issues": ["Hardcoded password detected, which is insecure."], "suggestions": ["Use secure password storage and verification methods."], "overall_security_rating": "High Risk" }

Now analyze this Java code:

String query = "SELECT * FROM users WHERE id = " + userId; executeQuery(query);

üéØ Multi-Shot Prompting System Prompt: You are a code security auditor. Analyze code snippets for vulnerabilities and suggest fixes. Return responses as JSON.

User Prompt (with multiple examples):

Example 1 (JavaScript):

document.write(userInput);

Expected Output:

{ "issues": ["Potential Cross-Site Scripting (XSS) vulnerability."], "suggestions": ["Use proper input sanitization before rendering user content."], "overall_security_rating": "Medium Risk" }

Example 2 (C++):

char buffer[10]; strcpy(buffer, input);

Expected Output:

{ "issues": ["Buffer overflow risk due to unsafe strcpy usage."], "suggestions": ["Use safer functions like strncpy or bounds checking."], "overall_security_rating": "High Risk" }

Now review this Python snippet:

import os os.system("rm -rf " + user_input)

üîπ Dynamic Prompting System Prompt Template: You are a code security auditor. Analyze the following {{language}} code snippet, detect vulnerabilities, and suggest fixes. Return JSON with issues, suggestions, and overall_security_rating.

User Prompt (dynamic):

Review this {{language}} code:

{{code_snippet}}

Example with Python snippet:

Review this Python code:

def process(data): exec(data)

üéØ Chain-of-Thought Prompting System Prompt: You are a code security auditor. Think step-by-step internally about possible security issues in the code, then output only the JSON result with issues, suggestions, and overall_security_rating. Do not show your reasoning.

User Prompt: Review the following Python code:

def authenticate(user_input): if user_input == "admin": grant_access()

Expected output:

{ "issues": ["Hardcoded admin username check without proper authentication."], "suggestions": ["Implement secure authentication mechanisms with password hashing."], "overall_security_rating": "High Risk" }

üß™ Evaluation Dataset & Testing Framework

We created a dataset of 5+ code snippets with known security vulnerabilities and expected outputs to benchmark CodeGuard.

üìÇ Evaluation Dataset (5 Samples)

[ { "id": 1, "language": "Python", "code": "def login(user_input): query = "SELECT * FROM users WHERE username = '" + user_input + "'"; execute(query)", "expected": "SQL Injection vulnerability due to unsafe string concatenation." }, { "id": 2, "language": "JavaScript", "code": "eval(userInput);", "expected": "Use of eval on user input leads to remote code execution risk." }, { "id": 3, "language": "Java", "code": "String password = "12345";", "expected": "Hardcoded password detected." }, { "id": 4, "language": "C++", "code": "char buf[8]; strcpy(buf, input);", "expected": "Buffer overflow risk with unsafe strcpy." }, { "id": 5, "language": "Python", "code": "os.system('rm -rf ' + user_input)", "expected": "Command injection vulnerability due to unsafe shell command." } ]

üßë‚Äç‚öñÔ∏è Judge Prompt You are a judge. Compare AI output to expected results on:

Correctness ‚Äì Did the model detect the main security flaw?

Completeness ‚Äì Did it suggest meaningful fixes?

Format ‚Äì Is the output valid JSON with all fields?

Clarity ‚Äì Is the feedback clear and actionable?

Return: Pass / Fail with justification.

‚öôÔ∏è Testing Framework Setup

for test in dataset: ai_output = run_model(test["code"]) verdict = judge(ai_output, test["expected"]) print(f"Test {test['id']}: {verdict}")

‚úÖ Why This Setup?

Objective and automated evaluation of security review accuracy.

Clear metrics on detection and suggestion quality.

Easily extendable with new vulnerability cases.